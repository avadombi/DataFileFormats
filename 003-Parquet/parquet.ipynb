{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parquet**\n",
    "\n",
    "`Parquet` is a columnar storage format that is optimized for distributed processing of large datasets. It is widely used in Big Data processing systems like `Hadoop` and `Apache Spark`. \n",
    "\n",
    "A partitioned parquet file is a parquet file that is partitioned into multiple smaller files based on the values of one or more columns. Partitioning can significantly improve query performance by allowing the processing system to read only the necessary files.\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "* **Parquet format**: A columnar storage format that is optimized for distributed processing of large datasets.\n",
    "\n",
    "* **Partitioning**: Dividing a dataset into smaller parts based on the values of one or more columns.\n",
    "\n",
    "* **Pandas DataFrame**: A two-dimensional labeled data structure with columns of potentially different types.\n",
    "\n",
    "* **pyarrow**: A Python package that provides a Python interface to the Arrow C++ library for working with columnar data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install pyarrow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import parquet from pyarrow as pa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Read parquet data using pyarrow.parquet (pa)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "ForecastSiteCode: int64\n",
       "ObservationTime: int64\n",
       "ObservationDate: timestamp[ms]\n",
       "WindDirection: int64\n",
       "WindSpeed: int64\n",
       "WindGust: double\n",
       "Visibility: double\n",
       "ScreenTemperature: double\n",
       "Pressure: double\n",
       "SignificantWeatherCode: int64\n",
       "SiteName: string\n",
       "Latitude: double\n",
       "Longitude: double\n",
       "Region: string\n",
       "Country: string\n",
       "----\n",
       "ForecastSiteCode: [[3002,3005,3008,3017,3023,...,3882,3002,3005,3008,3017],[3023,3026,3031,3034,3037,...,3797,3866,3872,3876,3882]]\n",
       "ObservationTime: [[0,0,0,0,0,...,12,13,13,13,13],[13,13,13,13,13,...,23,23,23,23,23]]\n",
       "ObservationDate: [[2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,...,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000],[2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,...,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000]]\n",
       "WindDirection: [[12,10,8,6,10,...,4,8,8,8,8],[9,9,10,8,8,...,1,0,1,1,1]]\n",
       "WindSpeed: [[8,2,6,8,30,...,5,19,18,19,19],[25,26,24,23,23,...,5,10,2,3,2]]\n",
       "WindGust: [[null,null,null,null,37,...,null,null,null,null,29],[36,41,37,34,37,...,null,null,null,null,null]]\n",
       "Visibility: [[30000,35000,50000,40000,2600,...,4000,8000,3500,11000,28000],[4600,9000,30000,10000,2900,...,22000,null,50000,null,35000]]\n",
       "ScreenTemperature: [[2.1,0.1,2.8,1.6,9.8,...,10,-99,7.4,8.1,9.2],[9.1,9.5,10.2,9.7,9.9,...,4.9,8.4,3.5,6.1,3.7]]\n",
       "Pressure: [[997,997,997,996,991,...,1030,null,1019,1020,1019],[1019,1018,1020,1020,1021,...,1019,1018,1019,1019,1019]]\n",
       "SignificantWeatherCode: [[8,7,-99,8,11,...,1,5,15,12,7],[15,12,12,12,15,...,0,-99,0,-99,0]]\n",
       "..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file path\n",
    "parquet_path = './weather.2016.parquet'\n",
    "\n",
    "# read\n",
    "table = pa.read_table(parquet_path)\n",
    "\n",
    "# print\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'people': [{'name': 'Naruto Shippuden', 'phone': '615-555-7164', 'emails': ['ns@gmail.com', 'n.s@gmail.com'], 'has_license': False}, {'name': 'Spider Man', 'phone': '560-555-5153', 'emails': None, 'has_license': True}]}\n"
     ]
    }
   ],
   "source": [
    "# import json (already in the built-in python)\n",
    "import json\n",
    "\n",
    "# create a json string object\n",
    "people_string = \"\"\"\n",
    "{\n",
    "    \"people\": [\n",
    "        {\n",
    "            \"name\": \"Naruto Shippuden\",\n",
    "            \"phone\": \"615-555-7164\",\n",
    "            \"emails\": [\"ns@gmail.com\", \"n.s@gmail.com\"],\n",
    "            \"has_license\": false\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Spider Man\",\n",
    "            \"phone\": \"560-555-5153\",\n",
    "            \"emails\": null,\n",
    "            \"has_license\": true\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# parse this string json to a python dictionary\n",
    "data = json.loads(people_string)\n",
    "\n",
    "# print the type\n",
    "print(type(data))  # output: <class 'dict'>\n",
    "\n",
    "# print the data\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Naruto Shippuden\n",
      "Name: Spider Man\n",
      "{\n",
      "  \"people\": [\n",
      "    {\n",
      "      \"emails\": [\n",
      "        \"ns@gmail.com\",\n",
      "        \"n.s@gmail.com\"\n",
      "      ],\n",
      "      \"has_license\": false,\n",
      "      \"name\": \"Naruto Shippuden\"\n",
      "    },\n",
      "    {\n",
      "      \"emails\": null,\n",
      "      \"has_license\": true,\n",
      "      \"name\": \"Spider Man\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# access each element of our dictionary (converted json object)\n",
    "for person in data['people']:\n",
    "    print(f'Name: {person['name']}')\n",
    "\n",
    "# delete phone key and its value for all people\n",
    "for person in data['people']:\n",
    "    del person['phone']\n",
    "\n",
    "# create a new json string object from the modified `data`\n",
    "# indent here is for formating the string\n",
    "# sort_keys: to sort the keys in ascending order\n",
    "new_string = json.dumps(data, indent=2, sort_keys=True)\n",
    "\n",
    "# print\n",
    "print(f'{new_string}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load a json file into a Python object and vice-versa**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Alabama, abbrev.: AL\n",
      "name: Alaska, abbrev.: AK\n",
      "name: Arizona, abbrev.: AZ\n",
      "name: Arkansas, abbrev.: AR\n",
      "name: California, abbrev.: CA\n",
      "name: Colorado, abbrev.: CO\n",
      "name: Connecticut, abbrev.: CT\n",
      "name: Delaware, abbrev.: DE\n",
      "name: Florida, abbrev.: FL\n",
      "name: Georgia, abbrev.: GA\n",
      "name: Hawaii, abbrev.: HI\n",
      "name: Idaho, abbrev.: ID\n",
      "name: Illinois, abbrev.: IL\n",
      "name: Indiana, abbrev.: IN\n",
      "name: Iowa, abbrev.: IA\n",
      "name: Kansas, abbrev.: KS\n",
      "name: Kentucky, abbrev.: KY\n",
      "name: Louisiana, abbrev.: LA\n",
      "name: Maine, abbrev.: ME\n",
      "name: Maryland, abbrev.: MD\n",
      "name: Massachusetts, abbrev.: MA\n",
      "name: Michigan, abbrev.: MI\n",
      "name: Minnesota, abbrev.: MN\n",
      "name: Mississippi, abbrev.: MS\n",
      "name: Missouri, abbrev.: MO\n",
      "name: Montana, abbrev.: MT\n",
      "name: Nebraska, abbrev.: NE\n",
      "name: Nevada, abbrev.: NV\n",
      "name: New Hampshire, abbrev.: NH\n",
      "name: New Jersey, abbrev.: NJ\n",
      "name: New Mexico, abbrev.: NM\n",
      "name: New York, abbrev.: NY\n",
      "name: North Carolina, abbrev.: NC\n",
      "name: North Dakota, abbrev.: ND\n",
      "name: Ohio, abbrev.: OH\n",
      "name: Oklahoma, abbrev.: OK\n",
      "name: Oregon, abbrev.: OR\n",
      "name: Pennsylvania, abbrev.: PA\n",
      "name: Rhode Island, abbrev.: RI\n",
      "name: South Carolina, abbrev.: SC\n",
      "name: South Dakota, abbrev.: SD\n",
      "name: Tennessee, abbrev.: TN\n",
      "name: Texas, abbrev.: TX\n",
      "name: Utah, abbrev.: UT\n",
      "name: Vermont, abbrev.: VT\n",
      "name: Virginia, abbrev.: VA\n",
      "name: Washington, abbrev.: WA\n",
      "name: West Virginia, abbrev.: WV\n",
      "name: Wisconsin, abbrev.: WI\n",
      "name: Wyoming, abbrev.: WY\n"
     ]
    }
   ],
   "source": [
    "# import json library\n",
    "import json\n",
    "\n",
    "# open the file (r for read)\n",
    "with open('states.json', 'r') as json_file:\n",
    "    # now it is open, we can load it into a Python object\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# loop through our file\n",
    "for state in data['states']:\n",
    "    print(f'name: {state['name']}, abbrev.: {state['abbreviation']}')\n",
    "\n",
    "# let's remove the area_code key and its value\n",
    "for state in data['states']:\n",
    "    del state['area_codes']\n",
    "\n",
    "# write the modified data to a json file (w for write)\n",
    "with open('new_states.json', 'w') as new_file:\n",
    "    # it dumps `data` and write it to 'new_states.json' (new_file)\n",
    "    json.dump(data, new_file, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Real-world example: retrieving json data from a public API (Github API)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"current_user_url\": \"https://api.github.com/user\",\n",
      "  \"current_user_authorizations_html_url\": \"https://github.com/settings/connections/applications{/client_id}\",\n",
      "  \"authorizations_url\": \"https://api.github.com/authorizations\",\n",
      "  \"code_search_url\": \"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\n",
      "  \"commit_search_url\": \"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\n",
      "  \"emails_url\": \"https://api.github.com/user/emails\",\n",
      "  \"emojis_url\": \"https://api.github.com/emojis\",\n",
      "  \"events_url\": \"https://api.github.com/events\",\n",
      "  \"feeds_url\": \"https://api.github.com/feeds\",\n",
      "  \"followers_url\": \"https://api.github.com/user/followers\",\n",
      "  \"following_url\": \"https://api.github.com/user/following{/target}\",\n",
      "  \"gists_url\": \"https://api.github.com/gists{/gist_id}\",\n",
      "  \"hub_url\": \"https://api.github.com/hub\",\n",
      "  \"issue_search_url\": \"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\n",
      "  \"issues_url\": \"https://api.github.com/issues\",\n",
      "  \"keys_url\": \"https://api.github.com/user/keys\",\n",
      "  \"label_search_url\": \"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\n",
      "  \"notifications_url\": \"https://api.github.com/notifications\",\n",
      "  \"organization_url\": \"https://api.github.com/orgs/{org}\",\n",
      "  \"organization_repositories_url\": \"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\n",
      "  \"organization_teams_url\": \"https://api.github.com/orgs/{org}/teams\",\n",
      "  \"public_gists_url\": \"https://api.github.com/gists/public\",\n",
      "  \"rate_limit_url\": \"https://api.github.com/rate_limit\",\n",
      "  \"repository_url\": \"https://api.github.com/repos/{owner}/{repo}\",\n",
      "  \"repository_search_url\": \"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\n",
      "  \"current_user_repositories_url\": \"https://api.github.com/user/repos{?type,page,per_page,sort}\",\n",
      "  \"starred_url\": \"https://api.github.com/user/starred{/owner}{/repo}\",\n",
      "  \"starred_gists_url\": \"https://api.github.com/gists/starred\",\n",
      "  \"topic_search_url\": \"https://api.github.com/search/topics?q={query}{&page,per_page}\",\n",
      "  \"user_url\": \"https://api.github.com/users/{user}\",\n",
      "  \"user_organizations_url\": \"https://api.github.com/user/orgs\",\n",
      "  \"user_repositories_url\": \"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\n",
      "  \"user_search_url\": \"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"\n",
      "}\n",
      "2395\n"
     ]
    }
   ],
   "source": [
    "# import json and urlopen (from urllib.request)\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# retrieve data\n",
    "url = \"https://api.github.com\"\n",
    "\n",
    "with urlopen(url) as response:\n",
    "    # retrieve the response and store it in source variable\n",
    "    source = response.read()  # output: a string\n",
    "\n",
    "# load it into a Python object\n",
    "data = json.loads(source)\n",
    "\n",
    "# dumps it\n",
    "data_indent = json.dumps(data, indent=2)\n",
    "\n",
    "# print\n",
    "print(data_indent)\n",
    "\n",
    "# print number of key-values\n",
    "print(len(data_indent))  # output: 2395"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
